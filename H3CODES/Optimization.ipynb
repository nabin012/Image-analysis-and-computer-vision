{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. SETUP AND MOUNT GOOGLE DRIVE ---\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully. ✅\")\n",
        "\n",
        "BASE_PATH = '/content/drive/My Drive/Spoof_data'\n",
        "ORIGINAL_TRAIN_DIR = os.path.join(BASE_PATH, 'train')\n",
        "ORIGINAL_TEST_DIR = os.path.join(BASE_PATH, 'test')\n",
        "\n",
        "IMG_WIDTH, IMG_HEIGHT = 128, 128\n",
        "CHANNELS = 3\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5 # REDUCED EPOCHS from 10 to 5 for faster execution.\n",
        "CLASS_NAMES = ['live', 'spoof']\n",
        "\n",
        "if not os.path.exists(ORIGINAL_TRAIN_DIR) or not os.path.exists(ORIGINAL_TEST_DIR):\n",
        "    raise FileNotFoundError(f\"Check your Google Drive path. Train or Test folder missing in {BASE_PATH}\")\n",
        "\n",
        "# --- 2. IMAGE PREPROCESSING FUNCTIONS ---\n",
        "HE_SUFFIX = '_he'\n",
        "BLUR_SUFFIX = '_blur'\n",
        "SHARPEN_SUFFIX = '_sharpen'\n",
        "DATASETS = {}\n",
        "\n",
        "def ensure_directory_exists(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def process_image(input_path, output_path, technique_name):\n",
        "    img = cv2.imread(input_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        return\n",
        "    processed_img = img.copy()\n",
        "\n",
        "    if technique_name == \"HE\":\n",
        "        processed_img = cv2.equalizeHist(img)\n",
        "    elif technique_name == \"BLUR\":\n",
        "        processed_img = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    elif technique_name == \"SHARPEN\":\n",
        "        kernel = np.array([[-1, -1, -1],\n",
        "                           [-1,  9, -1],\n",
        "                           [-1, -1, -1]])\n",
        "        processed_img = cv2.filter2D(img, -1, kernel)\n",
        "\n",
        "    processed_img_3channel = cv2.cvtColor(processed_img, cv2.COLOR_GRAY2BGR)\n",
        "    cv2.imwrite(output_path, processed_img_3channel)\n",
        "\n",
        "def run_preprocessing_pipeline(base_input_dir, base_output_dir, technique, technique_name):\n",
        "    print(f\"\\n--- Starting {technique_name} Processing for {os.path.basename(base_input_dir)} data ---\")\n",
        "    live_input = os.path.join(base_input_dir, \"live\")\n",
        "    spoof_input = os.path.join(base_input_dir, \"spoof\")\n",
        "    live_output = os.path.join(base_output_dir, \"live\")\n",
        "    spoof_output = os.path.join(base_output_dir, \"spoof\")\n",
        "    ensure_directory_exists(live_output)\n",
        "    ensure_directory_exists(spoof_output)\n",
        "\n",
        "    image_paths = []\n",
        "    for ext in ['jpg','jpeg','png','bmp']:\n",
        "        image_paths.extend(glob.glob(os.path.join(live_input, f\"*.{ext}\")))\n",
        "        image_paths.extend(glob.glob(os.path.join(spoof_input, f\"*.{ext}\")))\n",
        "\n",
        "    for input_path in tqdm(image_paths, desc=f\"Applying {technique_name}\"):\n",
        "        class_folder = os.path.basename(os.path.dirname(input_path))\n",
        "        filename = os.path.basename(input_path)\n",
        "        output_folder = live_output if class_folder==\"live\" else spoof_output\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "        process_image(input_path, output_path, technique)\n",
        "    print(f\"Successfully processed and saved files to {base_output_dir}\")\n",
        "\n",
        "# --- 3. CNN MODEL ---\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def create_spoof_detection_cnn(input_shape, final_dropout=0.5):\n",
        "    model = Sequential([\n",
        "        Conv2D(32,(3,3),activation='relu',input_shape=input_shape,padding='same'),\n",
        "        Conv2D(32,(3,3),activation='relu',padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2,2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Conv2D(64,(3,3),activation='relu',padding='same'),\n",
        "        Conv2D(64,(3,3),activation='relu',padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2,2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(512,activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(final_dropout),\n",
        "        Dense(1,activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# --- 5. TRAIN AND EVALUATE ---\n",
        "def train_and_evaluate_model(run_name, train_dir, test_dir, learning_rate, dropout_rate):\n",
        "    \"\"\"Trains and evaluates the model, returning all necessary data for later visualization.\"\"\"\n",
        "    tf.keras.backend.clear_session()\n",
        "    print(f\"\\n=== RUN: {run_name} | LR={learning_rate:.1e}, Dropout={dropout_rate:.2f} ===\")\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255, rotation_range=20, width_shift_range=0.1,\n",
        "        height_shift_range=0.1, horizontal_flip=True\n",
        "    )\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir, target_size=(IMG_WIDTH,IMG_HEIGHT), batch_size=BATCH_SIZE,\n",
        "        class_mode='binary', color_mode='rgb', classes=CLASS_NAMES, shuffle=True\n",
        "    )\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir, target_size=(IMG_WIDTH,IMG_HEIGHT), batch_size=BATCH_SIZE,\n",
        "        class_mode='binary', color_mode='rgb', classes=CLASS_NAMES, shuffle=False\n",
        "    )\n",
        "\n",
        "    input_shape = (IMG_WIDTH, IMG_HEIGHT, CHANNELS)\n",
        "    model = create_spoof_detection_cnn(input_shape, final_dropout=dropout_rate)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
        "    STEP_SIZE_TEST = test_generator.n // test_generator.batch_size\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_generator,\n",
        "        validation_steps=STEP_SIZE_TEST+1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    loss, accuracy = model.evaluate(test_generator, steps=STEP_SIZE_TEST+1, verbose=0)\n",
        "    predictions = model.predict(test_generator, verbose=0)\n",
        "    y_pred_classes = (predictions>0.5).astype(int).flatten()\n",
        "    y_true = test_generator.classes\n",
        "    if len(y_true)>len(y_pred_classes): y_true = y_true[:len(y_pred_classes)]\n",
        "\n",
        "    print(f\"\\nTest Loss: {loss:.4f} | Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "    # NOTE: Visualization is now called only for the best model in the main block (section 6)\n",
        "    return accuracy, history, y_true, y_pred_classes\n",
        "\n",
        "# --- 6. MAIN EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    preprocessing_configs = [(\"HE\", HE_SUFFIX), (\"BLUR\", BLUR_SUFFIX), (\"SHARPEN\", SHARPEN_SUFFIX)]\n",
        "\n",
        "    # Generate processed datasets\n",
        "    for technique, suffix in preprocessing_configs:\n",
        "        train_out = os.path.join(BASE_PATH,f'train{suffix}')\n",
        "        run_preprocessing_pipeline(ORIGINAL_TRAIN_DIR, train_out, technique, technique)\n",
        "        DATASETS[f'{technique}_train'] = train_out\n",
        "\n",
        "        test_out = os.path.join(BASE_PATH,f'test{suffix}')\n",
        "        run_preprocessing_pipeline(ORIGINAL_TEST_DIR, test_out, technique, technique)\n",
        "        DATASETS[f'{technique}_test'] = test_out\n",
        "\n",
        "    DATASETS['Original'] = (ORIGINAL_TRAIN_DIR, ORIGINAL_TEST_DIR)\n",
        "\n",
        "    # REDUCED GRID: 4 combinations (4 datasets * 4 combinations = 16 total runs)\n",
        "    tuning_grid = {\n",
        "        'learning_rates':[1e-3, 1e-4], # Reduced from 3 to 2\n",
        "        'dropout_rates':[0.4, 0.6]     # Reduced from 3 to 2\n",
        "    }\n",
        "    results = []\n",
        "\n",
        "    for dataset_name in ['Original','HE','Blur','Sharpen']:\n",
        "        if dataset_name=='Original':\n",
        "            TRAIN_DIR, TEST_DIR = ORIGINAL_TRAIN_DIR, ORIGINAL_TEST_DIR\n",
        "        else:\n",
        "            suffix = {'HE':HE_SUFFIX,'Blur':BLUR_SUFFIX,'Sharpen':SHARPEN_SUFFIX}[dataset_name]\n",
        "            TRAIN_DIR = os.path.join(BASE_PATH,f'train{suffix}')\n",
        "            TEST_DIR = os.path.join(BASE_PATH,f'test{suffix}')\n",
        "\n",
        "        for lr in tuning_grid['learning_rates']:\n",
        "            for dr in tuning_grid['dropout_rates']:\n",
        "                run_name = f\"{dataset_name}_LR={lr:.1e}_DO={dr:.2f}\"\n",
        "\n",
        "                # Call train function and capture all data\n",
        "                acc, history, y_true, y_pred_classes = train_and_evaluate_model(\n",
        "                    run_name, TRAIN_DIR, TEST_DIR, lr, dr\n",
        "                )\n",
        "\n",
        "                results.append({\n",
        "                    'dataset': dataset_name,\n",
        "                    'lr': lr,\n",
        "                    'dropout': dr,\n",
        "                    'accuracy': acc,\n",
        "                    'run_name': run_name,\n",
        "                    'history': history,\n",
        "                    'y_true': y_true,\n",
        "                    'y_pred_classes': y_pred_classes\n",
        "                })\n",
        "\n",
        "    # --- Summary ---\n",
        "    print(\"\\n\\n=== FINAL COMBINED TUNING & PREPROCESSING SUMMARY ===\")\n",
        "    sorted_results = sorted(results, key=lambda x:x['accuracy'], reverse=True)\n",
        "    print(\"Rank | Dataset   | LR     | Dropout | Test Accuracy\")\n",
        "    print(\"-\"*65)\n",
        "\n",
        "    for i,r in enumerate(sorted_results):\n",
        "        print(f\"{i+1:<4} | {r['dataset']:<9} | {r['lr']:.1e} | {r['dropout']:.2f} | {r['accuracy']*100:.2f}%\")\n",
        "\n",
        "    # --- VISUALIZE THE BEST MODEL (Rank 1) ---\n",
        "    best_run = sorted_results[0]\n",
        "    print(f\"\\n\\n--- VISUALIZING BEST MODEL: {best_run['run_name']} ---\")\n",
        "    visualize_results(\n",
        "        best_run['history'],\n",
        "        best_run['y_true'],\n",
        "        best_run['y_pred_classes'],\n",
        "        best_run['run_name']\n",
        "    )\n",
        "    print(\"\\nTraining and analysis complete. The visualization above shows the detailed performance of the best combination of preprocessing technique and hyperparameters.\")\n"
      ],
      "metadata": {
        "id": "Y_QuPz4J5gmF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4508a482-4133-4823-aa7e-4c4518c16332"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully. ✅\n",
            "\n",
            "--- Starting HE Processing for train data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Applying HE: 100%|██████████| 407/407 [00:11<00:00, 34.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed and saved files to /content/drive/My Drive/Spoof_data/train_he\n",
            "\n",
            "--- Starting HE Processing for test data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Applying HE: 100%|██████████| 400/400 [00:11<00:00, 34.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed and saved files to /content/drive/My Drive/Spoof_data/test_he\n",
            "\n",
            "--- Starting BLUR Processing for train data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Applying BLUR: 100%|██████████| 407/407 [00:10<00:00, 40.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed and saved files to /content/drive/My Drive/Spoof_data/train_blur\n",
            "\n",
            "--- Starting BLUR Processing for test data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Applying BLUR: 100%|██████████| 400/400 [00:10<00:00, 38.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed and saved files to /content/drive/My Drive/Spoof_data/test_blur\n",
            "\n",
            "--- Starting SHARPEN Processing for train data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Applying SHARPEN: 100%|██████████| 407/407 [00:11<00:00, 34.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed and saved files to /content/drive/My Drive/Spoof_data/train_sharpen\n",
            "\n",
            "--- Starting SHARPEN Processing for test data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Applying SHARPEN: 100%|██████████| 400/400 [00:11<00:00, 33.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed and saved files to /content/drive/My Drive/Spoof_data/test_sharpen\n",
            "\n",
            "=== RUN: Original_LR=1.0e-03_DO=0.40 | LR=1.0e-03, Dropout=0.40 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6906 - loss: 0.8760 - val_accuracy: 0.5000 - val_loss: 1.6294\n",
            "Epoch 2/5\n",
            "\u001b[1m 1/12\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7500 - loss: 0.4500"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 248ms/step - accuracy: 0.7500 - loss: 0.4500 - val_accuracy: 0.5000 - val_loss: 1.5991\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 640ms/step - accuracy: 0.8476 - loss: 0.3436 - val_accuracy: 0.5000 - val_loss: 1.7536\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 327ms/step - accuracy: 0.9062 - loss: 0.2911 - val_accuracy: 0.5000 - val_loss: 1.7869\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 616ms/step - accuracy: 0.9048 - loss: 0.2350 - val_accuracy: 0.5000 - val_loss: 1.7041\n",
            "\n",
            "Test Loss: 1.7041 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: Original_LR=1.0e-03_DO=0.60 | LR=1.0e-03, Dropout=0.60 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.6681 - loss: 1.0603 - val_accuracy: 0.5000 - val_loss: 1.5771\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 247ms/step - accuracy: 0.8125 - loss: 0.4084 - val_accuracy: 0.5000 - val_loss: 1.8694\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 632ms/step - accuracy: 0.8124 - loss: 0.4453 - val_accuracy: 0.5000 - val_loss: 1.8081\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 325ms/step - accuracy: 0.9062 - loss: 0.1936 - val_accuracy: 0.5000 - val_loss: 1.8394\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 612ms/step - accuracy: 0.8457 - loss: 0.3437 - val_accuracy: 0.5000 - val_loss: 12.9835\n",
            "\n",
            "Test Loss: 12.9835 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: Original_LR=1.0e-04_DO=0.40 | LR=1.0e-04, Dropout=0.40 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6563 - loss: 0.8625 - val_accuracy: 0.5000 - val_loss: 1.1430\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 306ms/step - accuracy: 0.6562 - loss: 1.0055 - val_accuracy: 0.5000 - val_loss: 1.2707\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 613ms/step - accuracy: 0.8739 - loss: 0.3348 - val_accuracy: 0.5000 - val_loss: 2.3426\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 276ms/step - accuracy: 0.8125 - loss: 0.3525 - val_accuracy: 0.5000 - val_loss: 2.3928\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 658ms/step - accuracy: 0.8708 - loss: 0.2673 - val_accuracy: 0.5000 - val_loss: 2.9407\n",
            "\n",
            "Test Loss: 2.9407 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: Original_LR=1.0e-04_DO=0.60 | LR=1.0e-04, Dropout=0.60 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6341 - loss: 1.0706 - val_accuracy: 0.5000 - val_loss: 0.7226\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 466ms/step - accuracy: 0.8438 - loss: 0.3566 - val_accuracy: 0.5000 - val_loss: 0.7446\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 598ms/step - accuracy: 0.7804 - loss: 0.5094 - val_accuracy: 0.5000 - val_loss: 1.7361\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 246ms/step - accuracy: 0.8438 - loss: 0.4786 - val_accuracy: 0.5000 - val_loss: 1.7829\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 686ms/step - accuracy: 0.8730 - loss: 0.3304 - val_accuracy: 0.5000 - val_loss: 2.1476\n",
            "\n",
            "Test Loss: 2.1476 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: HE_LR=1.0e-03_DO=0.40 | LR=1.0e-03, Dropout=0.40 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6130 - loss: 1.1592 - val_accuracy: 0.5000 - val_loss: 0.7478\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 315ms/step - accuracy: 0.6562 - loss: 0.7070 - val_accuracy: 0.5000 - val_loss: 0.7504\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 695ms/step - accuracy: 0.7846 - loss: 0.4200 - val_accuracy: 0.5075 - val_loss: 0.7700\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 276ms/step - accuracy: 0.6562 - loss: 0.8496 - val_accuracy: 0.5150 - val_loss: 0.8368\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 738ms/step - accuracy: 0.7987 - loss: 0.4665 - val_accuracy: 0.4950 - val_loss: 4.3342\n",
            "\n",
            "Test Loss: 4.3342 | Test Accuracy: 49.50%\n",
            "\n",
            "=== RUN: HE_LR=1.0e-03_DO=0.60 | LR=1.0e-03, Dropout=0.60 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6064 - loss: 1.2204 - val_accuracy: 0.5000 - val_loss: 0.7243\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 316ms/step - accuracy: 0.7812 - loss: 0.4300 - val_accuracy: 0.5000 - val_loss: 0.7397\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 697ms/step - accuracy: 0.8639 - loss: 0.4718 - val_accuracy: 0.7475 - val_loss: 0.5582\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 312ms/step - accuracy: 0.6562 - loss: 0.5978 - val_accuracy: 0.7575 - val_loss: 0.5530\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 740ms/step - accuracy: 0.8651 - loss: 0.3943 - val_accuracy: 0.6350 - val_loss: 0.6159\n",
            "\n",
            "Test Loss: 0.6159 | Test Accuracy: 63.50%\n",
            "\n",
            "=== RUN: HE_LR=1.0e-04_DO=0.40 | LR=1.0e-04, Dropout=0.40 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6444 - loss: 0.8154 - val_accuracy: 0.5000 - val_loss: 0.7606\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 346ms/step - accuracy: 0.5312 - loss: 0.8956 - val_accuracy: 0.5000 - val_loss: 0.7734\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 662ms/step - accuracy: 0.7949 - loss: 0.5193 - val_accuracy: 0.5000 - val_loss: 0.9992\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 284ms/step - accuracy: 0.7500 - loss: 0.4197 - val_accuracy: 0.5000 - val_loss: 1.0196\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 697ms/step - accuracy: 0.8402 - loss: 0.3939 - val_accuracy: 0.5000 - val_loss: 1.4415\n",
            "\n",
            "Test Loss: 1.4415 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: HE_LR=1.0e-04_DO=0.60 | LR=1.0e-04, Dropout=0.60 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.6339 - loss: 0.8350 - val_accuracy: 0.4850 - val_loss: 0.6977\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 466ms/step - accuracy: 0.7500 - loss: 0.4234 - val_accuracy: 0.4675 - val_loss: 0.7010\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 626ms/step - accuracy: 0.7888 - loss: 0.4767 - val_accuracy: 0.5000 - val_loss: 0.7033\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 287ms/step - accuracy: 0.9375 - loss: 0.2887 - val_accuracy: 0.5000 - val_loss: 0.7041\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 682ms/step - accuracy: 0.8457 - loss: 0.3870 - val_accuracy: 0.5000 - val_loss: 0.7623\n",
            "\n",
            "Test Loss: 0.7623 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: Blur_LR=1.0e-03_DO=0.40 | LR=1.0e-03, Dropout=0.40 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.6799 - loss: 1.0069 - val_accuracy: 0.5000 - val_loss: 0.8637\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 261ms/step - accuracy: 0.8125 - loss: 0.3210 - val_accuracy: 0.5000 - val_loss: 0.8658\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 651ms/step - accuracy: 0.8529 - loss: 0.4063 - val_accuracy: 0.5025 - val_loss: 0.6987\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 239ms/step - accuracy: 0.9062 - loss: 0.1570 - val_accuracy: 0.5025 - val_loss: 0.7126\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 693ms/step - accuracy: 0.8933 - loss: 0.3097 - val_accuracy: 0.4950 - val_loss: 1.2001\n",
            "\n",
            "Test Loss: 1.2001 | Test Accuracy: 49.50%\n",
            "\n",
            "=== RUN: Blur_LR=1.0e-03_DO=0.60 | LR=1.0e-03, Dropout=0.60 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6677 - loss: 1.0866 - val_accuracy: 0.5900 - val_loss: 0.6275\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 248ms/step - accuracy: 0.8125 - loss: 0.4876 - val_accuracy: 0.5875 - val_loss: 0.6275\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 709ms/step - accuracy: 0.8119 - loss: 0.3760 - val_accuracy: 0.5000 - val_loss: 0.9678\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 248ms/step - accuracy: 0.7812 - loss: 0.5118 - val_accuracy: 0.5000 - val_loss: 0.9252\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 627ms/step - accuracy: 0.8394 - loss: 0.3574 - val_accuracy: 0.5000 - val_loss: 1.4847\n",
            "\n",
            "Test Loss: 1.4847 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: Blur_LR=1.0e-04_DO=0.40 | LR=1.0e-04, Dropout=0.40 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6747 - loss: 0.9832 - val_accuracy: 0.5000 - val_loss: 0.7563\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 244ms/step - accuracy: 0.8438 - loss: 0.4520 - val_accuracy: 0.5000 - val_loss: 0.7796\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 623ms/step - accuracy: 0.8717 - loss: 0.3745 - val_accuracy: 0.5000 - val_loss: 1.2358\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 297ms/step - accuracy: 0.8750 - loss: 0.3514 - val_accuracy: 0.5000 - val_loss: 1.2786\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 617ms/step - accuracy: 0.8357 - loss: 0.3592 - val_accuracy: 0.5000 - val_loss: 1.7059\n",
            "\n",
            "Test Loss: 1.7059 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: Blur_LR=1.0e-04_DO=0.60 | LR=1.0e-04, Dropout=0.60 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6803 - loss: 0.8046 - val_accuracy: 0.5000 - val_loss: 0.8451\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 298ms/step - accuracy: 0.7812 - loss: 0.7406 - val_accuracy: 0.5000 - val_loss: 0.9028\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 616ms/step - accuracy: 0.8736 - loss: 0.4345 - val_accuracy: 0.5000 - val_loss: 1.5267\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 467ms/step - accuracy: 0.9062 - loss: 0.1691 - val_accuracy: 0.5000 - val_loss: 1.5773\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 708ms/step - accuracy: 0.8216 - loss: 0.3735 - val_accuracy: 0.5000 - val_loss: 2.3356\n",
            "\n",
            "Test Loss: 2.3356 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: Sharpen_LR=1.0e-03_DO=0.40 | LR=1.0e-03, Dropout=0.40 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6800 - loss: 0.8619 - val_accuracy: 0.6025 - val_loss: 0.6192\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 299ms/step - accuracy: 0.8438 - loss: 0.4384 - val_accuracy: 0.5025 - val_loss: 0.6467\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 685ms/step - accuracy: 0.7937 - loss: 0.4672 - val_accuracy: 0.5000 - val_loss: 1.5631\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 271ms/step - accuracy: 0.8438 - loss: 0.3983 - val_accuracy: 0.5000 - val_loss: 1.6165\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 734ms/step - accuracy: 0.8508 - loss: 0.3717 - val_accuracy: 0.5000 - val_loss: 2.5758\n",
            "\n",
            "Test Loss: 2.5758 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: Sharpen_LR=1.0e-03_DO=0.60 | LR=1.0e-03, Dropout=0.60 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6587 - loss: 0.9239 - val_accuracy: 0.5075 - val_loss: 0.7156\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 298ms/step - accuracy: 0.7812 - loss: 0.4167 - val_accuracy: 0.5175 - val_loss: 0.6663\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 755ms/step - accuracy: 0.7800 - loss: 0.5071 - val_accuracy: 0.5000 - val_loss: 1.2252\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 265ms/step - accuracy: 0.8750 - loss: 0.3370 - val_accuracy: 0.5000 - val_loss: 1.4133\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 838ms/step - accuracy: 0.8080 - loss: 0.4470 - val_accuracy: 0.5000 - val_loss: 1.6183\n",
            "\n",
            "Test Loss: 1.6183 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: Sharpen_LR=1.0e-04_DO=0.40 | LR=1.0e-04, Dropout=0.40 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.7260 - loss: 0.9242 - val_accuracy: 0.7375 - val_loss: 0.6291\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 262ms/step - accuracy: 0.6875 - loss: 0.7359 - val_accuracy: 0.7900 - val_loss: 0.6189\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 722ms/step - accuracy: 0.8060 - loss: 0.5562 - val_accuracy: 0.5000 - val_loss: 1.4215\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 266ms/step - accuracy: 0.8125 - loss: 0.4642 - val_accuracy: 0.5000 - val_loss: 1.4941\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 673ms/step - accuracy: 0.8638 - loss: 0.3428 - val_accuracy: 0.5000 - val_loss: 2.0984\n",
            "\n",
            "Test Loss: 2.0984 | Test Accuracy: 50.00%\n",
            "\n",
            "=== RUN: Sharpen_LR=1.0e-04_DO=0.60 | LR=1.0e-04, Dropout=0.60 ===\n",
            "Found 407 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.6366 - loss: 0.8212 - val_accuracy: 0.5000 - val_loss: 0.7136\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 262ms/step - accuracy: 0.8125 - loss: 0.5546 - val_accuracy: 0.5000 - val_loss: 0.7850\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 714ms/step - accuracy: 0.8171 - loss: 0.4538 - val_accuracy: 0.5000 - val_loss: 1.8693\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 264ms/step - accuracy: 0.8696 - loss: 0.3554 - val_accuracy: 0.5000 - val_loss: 1.9408\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 660ms/step - accuracy: 0.8267 - loss: 0.4824 - val_accuracy: 0.5000 - val_loss: 2.8126\n",
            "\n",
            "Test Loss: 2.8126 | Test Accuracy: 50.00%\n",
            "\n",
            "\n",
            "=== FINAL COMBINED TUNING & PREPROCESSING SUMMARY ===\n",
            "Rank | Dataset   | LR     | Dropout | Test Accuracy\n",
            "-----------------------------------------------------------------\n",
            "1    | HE        | 1.0e-03 | 0.60 | 63.50%\n",
            "2    | Original  | 1.0e-03 | 0.40 | 50.00%\n",
            "3    | Original  | 1.0e-03 | 0.60 | 50.00%\n",
            "4    | Original  | 1.0e-04 | 0.40 | 50.00%\n",
            "5    | Original  | 1.0e-04 | 0.60 | 50.00%\n",
            "6    | HE        | 1.0e-04 | 0.40 | 50.00%\n",
            "7    | HE        | 1.0e-04 | 0.60 | 50.00%\n",
            "8    | Blur      | 1.0e-03 | 0.60 | 50.00%\n",
            "9    | Blur      | 1.0e-04 | 0.40 | 50.00%\n",
            "10   | Blur      | 1.0e-04 | 0.60 | 50.00%\n",
            "11   | Sharpen   | 1.0e-03 | 0.40 | 50.00%\n",
            "12   | Sharpen   | 1.0e-03 | 0.60 | 50.00%\n",
            "13   | Sharpen   | 1.0e-04 | 0.40 | 50.00%\n",
            "14   | Sharpen   | 1.0e-04 | 0.60 | 50.00%\n",
            "15   | HE        | 1.0e-03 | 0.40 | 49.50%\n",
            "16   | Blur      | 1.0e-03 | 0.40 | 49.50%\n",
            "\n",
            "\n",
            "--- VISUALIZING BEST MODEL: HE_LR=1.0e-03_DO=0.60 ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'visualize_results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1842833994.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mbest_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\n--- VISUALIZING BEST MODEL: {best_run['run_name']} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     visualize_results(\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mbest_run\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mbest_run\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'visualize_results' is not defined"
          ]
        }
      ]
    }
  ]
}
